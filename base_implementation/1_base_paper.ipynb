{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2588892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e1f16",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Project Summary\n",
    "\n",
    "- **Dataset**: HMDB51 Fight (75,900 images, 8 classes)\n",
    "- **Models**: Sequential CNN, MobileNetV2, VGG-16\n",
    "- **Best Result**: VGG-16 with 71% test accuracy\n",
    "- **Hardware**: 8x H100/H200 GPUs\n",
    "- **Goal**: Reproduce paper results with production-ready code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c284433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VIOLENCE DETECTION - DEEP LEARNING IMPLEMENTATION\n",
      "Hsairi et al. (2024)\n",
      "================================================================================\n",
      "PyTorch Version: 2.9.1+cu128\n",
      "CUDA Available: True\n",
      "Number of GPUs: 8\n",
      "GPU: NVIDIA H200\n",
      "  GPU 0: NVIDIA H200 - 150.1GB\n",
      "  GPU 1: NVIDIA H200 - 150.1GB\n",
      "  GPU 2: NVIDIA H200 - 150.1GB\n",
      "  GPU 3: NVIDIA H200 - 150.1GB\n",
      "  GPU 4: NVIDIA H200 - 150.1GB\n",
      "  GPU 5: NVIDIA H200 - 150.1GB\n",
      "  GPU 6: NVIDIA H200 - 150.1GB\n",
      "  GPU 7: NVIDIA H200 - 150.1GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================\n",
    "# Run in terminal BEFORE Jupyter:\n",
    "# conda create -n violence_detection python=3.10 -y\n",
    "# conda activate violence_detection\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# pip install numpy pandas scikit-learn opencv-python pillow matplotlib seaborn kaggle tensorboard tqdm jupyter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Vision\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_auc_score, top_k_accuracy_score\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VIOLENCE DETECTION - DEEP LEARNING IMPLEMENTATION\")\n",
    "print(\"Hsairi et al. (2024)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name} - {props.total_memory / 1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63891a4a",
   "metadata": {},
   "source": [
    "## CELL 2: Global Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "202d4973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  - Dataset path: ./fight_dataset/actions (2)/actions\n",
      "  - Image size: 224x224\n",
      "  - Epochs: 50\n",
      "  - Batch size per GPU: 64\n",
      "  - Total batch size (8 GPUs): 512\n",
      "  - VGG-16 FC units: 500\n",
      "  - VGG-16 Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GLOBAL CONFIGURATION - ALL HYPERPARAMETERS\n",
    "# ============================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Dataset - Updated to match actual folder structure\n",
    "    'dataset_path': './fight_dataset/actions (2)/actions',\n",
    "    'kaggle_dataset': 'anbumalar1991/fight-dataset',\n",
    "    'models_dir': './models',\n",
    "    'results_dir': './results',\n",
    "    \n",
    "    # Image processing\n",
    "    'image_size': 224,  # VGG-16 standard\n",
    "    'imagenet_mean': [0.485, 0.456, 0.406],\n",
    "    'imagenet_std': [0.229, 0.224, 0.225],\n",
    "    \n",
    "    # Data split (from paper)\n",
    "    'train_ratio': 0.72,\n",
    "    'val_ratio': 0.20,\n",
    "    'test_ratio': 0.08,\n",
    "    \n",
    "    # Classes (8 from HMDB51 Fight) - Updated with correct names\n",
    "    'classes': {\n",
    "        0: 'hit', 1: 'kick', 2: 'punch', 3: 'shoot_gun',\n",
    "        4: 'push', 5: 'ride_horse', 6: 'stand', 7: 'wave'\n",
    "    },\n",
    "\n",
    "    'num_classes': 8,\n",
    "    'violence_classes': [0, 1, 2, 3],\n",
    "    'non_violence_classes': [4, 5, 6, 7],\n",
    "    \n",
    "    # Multi-GPU\n",
    "    'num_gpus': torch.cuda.device_count(),\n",
    "    'distributed': torch.cuda.device_count() > 1,\n",
    "    'backend': 'nccl',\n",
    "    \n",
    "    # Training\n",
    "    'epochs': 50,\n",
    "    'early_stopping_patience': 10,\n",
    "    'lr_scheduler_patience': 5,\n",
    "    'lr_scheduler_factor': 0.5,\n",
    "    'batch_size_per_gpu': 64,  # Total = 64 * 8 = 512\n",
    "    \n",
    "    # S-CNN hyperparameters\n",
    "    'scnn': {\n",
    "        'learning_rate': 0.001,\n",
    "        'dropout_rates': [0.4, 0.4, 0.2],\n",
    "        'dense_units': [128, 64],\n",
    "        'conv_filters': [64, 128, 32, 64],\n",
    "    },\n",
    "    \n",
    "    # MobileNetV2 hyperparameters\n",
    "    'mobilenetv2': {\n",
    "        'learning_rate': 0.001,\n",
    "        'dropout_rate': 0.2,\n",
    "        'weight_decay': 1e-4,  # L2 regularization\n",
    "        'freeze_backbone': True,\n",
    "        'unfreeze_at_epoch': 20,\n",
    "    },\n",
    "    \n",
    "    # VGG-16 hyperparameters (CRITICAL!)\n",
    "    'vgg16': {\n",
    "        'learning_rate': 0.0001,  # NOT 0.001!\n",
    "        'fc_units': 500,  # NOT 2048 or 1000!\n",
    "        'dropout_rate': 0.1,\n",
    "        'weight_decay': 0.0,\n",
    "        'freeze_backbone': True,\n",
    "        'unfreeze_at_epoch': 20,\n",
    "    },\n",
    "    \n",
    "    # Other settings\n",
    "    'optimizer': 'adam',\n",
    "    'adam_betas': (0.9, 0.999),\n",
    "    'loss_function': 'crossentropy',\n",
    "    'label_smoothing': 0.1,\n",
    "    'random_seed': 42,\n",
    "    'deterministic': True,\n",
    "    'metrics': ['accuracy', 'auc', 'top_k'],\n",
    "    'top_k_values': [2, 3],\n",
    "    'save_plots': True,\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "Path(CONFIG['models_dir']).mkdir(exist_ok=True)\n",
    "Path(CONFIG['results_dir']).mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - Dataset path: {CONFIG['dataset_path']}\")\n",
    "print(f\"  - Image size: {CONFIG['image_size']}x{CONFIG['image_size']}\")\n",
    "print(f\"  - Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"  - Batch size per GPU: {CONFIG['batch_size_per_gpu']}\")\n",
    "print(f\"  - Total batch size (8 GPUs): {CONFIG['batch_size_per_gpu'] * 8}\")\n",
    "print(f\"  - VGG-16 FC units: {CONFIG['vgg16']['fc_units']}\")\n",
    "print(f\"  - VGG-16 Learning Rate: {CONFIG['vgg16']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17001615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset anbumalar1991/fight-dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/anbumalar1991/fight-dataset\n",
      "Downloading fight-dataset.zip to fight_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.80G/1.80G [00:01<00:00, 1.63GB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset downloaded and extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import kaggle\n",
    "\n",
    "# REPLACE THESE WITH YOUR ACTUAL KAGGLE CREDENTIALS\n",
    "os.environ['KAGGLE_USERNAME'] = \"Baari\" \n",
    "os.environ['KAGGLE_KEY'] = \"KGAT_df4cc0be6e9be3f8e28c2242065a2fec\"\n",
    "\n",
    "def download_dataset():\n",
    "    \"\"\"Download and extract the dataset from Kaggle if not present.\"\"\"\n",
    "    dataset_dir = Path(CONFIG['dataset_path'])\n",
    "    \n",
    "    # Check if dataset already exists and is not empty\n",
    "    if dataset_dir.exists() and any(dataset_dir.iterdir()):\n",
    "        print(f\"Dataset already exists at {dataset_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading dataset {CONFIG['kaggle_dataset']}...\")\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        auth = KaggleApi()\n",
    "        auth.authenticate()\n",
    "        \n",
    "        # Download and unzip\n",
    "        kaggle.api.dataset_download_files(\n",
    "            CONFIG['kaggle_dataset'],\n",
    "            path=dataset_dir,\n",
    "            unzip=True,\n",
    "            quiet=False\n",
    "        )\n",
    "        print(\"Dataset downloaded and extracted successfully.\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Error: 'kaggle' library not found. Please install it via 'pip install kaggle'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        print(\"Ensure you have placed your 'kaggle.json' API token in ~/.kaggle/\")\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635365da",
   "metadata": {},
   "source": [
    "## CELL 3: Reproducibility Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6484bf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seeds set to 42\n"
     ]
    }
   ],
   "source": [
    "def set_seeds(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    if CONFIG['deterministic']:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds(CONFIG['random_seed'])\n",
    "print(f\"Random seeds set to {CONFIG['random_seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b901a01d",
   "metadata": {},
   "source": [
    "## CELL 4: Data Loading & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d3f1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split:\n",
      "  - Training: 54439 images\n",
      "  - Validation: 13609 images\n",
      "  - Testing: 7807 images\n",
      "  - Total: 75855 images\n",
      "\n",
      "Class distribution in training set:\n",
      "  - hit: 2983 images\n",
      "  - kick: 4331 images\n",
      "  - punch: 7001 images\n",
      "  - push: 7634 images\n",
      "  - ride_horse: 12454 images\n",
      "  - shoot_gun: 7254 images\n",
      "  - stand: 8314 images\n",
      "  - wave: 4468 images\n"
     ]
    }
   ],
   "source": [
    "class FightDataset(Dataset):\n",
    "    \"\"\"Custom dataset for HMDB51 Fight images with augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths: List[str], labels: List[int], \n",
    "                 transform: Optional[transforms.Compose] = None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "def load_split_from_folder(base_path: Path, split: str) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Load images from train or test folder structure.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Root path containing train/test folders\n",
    "        split: Either 'train' or 'test'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (image_paths, labels)\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    class_to_idx = CONFIG['classes']\n",
    "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "    \n",
    "    split_path = base_path / split\n",
    "    \n",
    "    if not split_path.exists():\n",
    "        raise ValueError(f\"Split path does not exist: {split_path}\")\n",
    "    \n",
    "    for class_name, class_idx in idx_to_class.items():\n",
    "        class_dir = split_path / class_name\n",
    "        if class_dir.exists():\n",
    "            # Support both .jpg and .png files\n",
    "            for img_file in list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png')):\n",
    "                image_paths.append(str(img_file))\n",
    "                labels.append(class_idx)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "def create_data_loaders(dataset_path: str, batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Create training, validation, and test data loaders from train/test folders.\n",
    "    \n",
    "    This version works with pre-split train/test folders and creates a validation\n",
    "    set by splitting part of the training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_path = Path(dataset_path)\n",
    "    \n",
    "    # Load train and test sets\n",
    "    train_paths, train_labels = load_split_from_folder(base_path, 'train')\n",
    "    test_paths, test_labels = load_split_from_folder(base_path, 'test')\n",
    "    \n",
    "    # Split training set into train and validation\n",
    "    n_train = len(train_paths)\n",
    "    # Use 80% of train for actual training, 20% for validation\n",
    "    val_split_ratio = 0.2\n",
    "    val_size = int(val_split_ratio * n_train)\n",
    "    \n",
    "    # Shuffle training data\n",
    "    indices = np.arange(n_train)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    val_indices = indices[:val_size]\n",
    "    actual_train_indices = indices[val_size:]\n",
    "    \n",
    "    # Define transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomResizedCrop(size=CONFIG['image_size'], scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=CONFIG['imagenet_mean'], \n",
    "                           std=CONFIG['imagenet_std']),\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=CONFIG['imagenet_mean'], \n",
    "                           std=CONFIG['imagenet_std']),\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FightDataset(\n",
    "        [train_paths[i] for i in actual_train_indices],\n",
    "        [train_labels[i] for i in actual_train_indices],\n",
    "        train_transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = FightDataset(\n",
    "        [train_paths[i] for i in val_indices],\n",
    "        [train_labels[i] for i in val_indices],\n",
    "        val_test_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = FightDataset(\n",
    "        test_paths,\n",
    "        test_labels,\n",
    "        val_test_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"  - Training: {len(train_dataset)} images\")\n",
    "    print(f\"  - Validation: {len(val_dataset)} images\")\n",
    "    print(f\"  - Testing: {len(test_dataset)} images\")\n",
    "    print(f\"  - Total: {len(train_dataset) + len(val_dataset) + len(test_dataset)} images\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(f\"\\nClass distribution in training set:\")\n",
    "    train_class_counts = defaultdict(int)\n",
    "    for label in [train_labels[i] for i in actual_train_indices]:\n",
    "        train_class_counts[CONFIG['classes'][label]] += 1\n",
    "    for class_name, count in sorted(train_class_counts.items()):\n",
    "        print(f\"  - {class_name}: {count} images\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    CONFIG['dataset_path'],\n",
    "    CONFIG['batch_size_per_gpu']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a33c5",
   "metadata": {},
   "source": [
    "## CELL 5: Model Architecture - Sequential CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6f760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential CNN for violence detection.\n",
    "    \n",
    "    Architecture (from paper):\n",
    "    - Conv2D(64, 3x3) + ReLU + Conv2D(128, 3x3) + ReLU + MaxPool + Dropout(0.4)\n",
    "    - Conv2D(32, 3x3) + ReLU + MaxPool + Dropout(0.4)\n",
    "    - Conv2D(64, 3x3) + ReLU + Dropout(0.2) + MaxPool\n",
    "    - Flatten + Dense(128) + Dense(64) + Output(8)\n",
    "    \n",
    "    Key innovation: Strategic dropout placement prevents overfitting\n",
    "    Paper result: 63% test accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First block\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.4)  # After first maxpool\n",
    "        \n",
    "        # Second block\n",
    "        self.conv2_1 = nn.Conv2d(128, 32, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.4)  # After second maxpool\n",
    "        \n",
    "        # Third block\n",
    "        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout3 = nn.Dropout(0.2)  # Before third maxpool\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate flattened size (224 -> 56 -> 28 -> 14)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # First block\n",
    "        x = F.relu(self.conv1_1(x))\n",
    "        x = F.relu(self.conv1_2(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31820b34",
   "metadata": {},
   "source": [
    "## CELL 6: Model Architecture - MobileNetV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05102d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV2 with custom head for violence detection.\n",
    "    \n",
    "    Modifications (from paper):\n",
    "    - L2 regularization (weight_decay=1e-4)\n",
    "    - Batch normalization in custom head\n",
    "    - Dropout(0.2) for regularization\n",
    "    \n",
    "    Paper result: 69% test accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained MobileNetV2\n",
    "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove original classification head\n",
    "        self.backbone.classifier = nn.Sequential()\n",
    "        \n",
    "        # Get feature dimension (1280 for MobileNetV2)\n",
    "        num_features = 1280\n",
    "        \n",
    "        # Custom classifier head with batch norm and dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Dropout(0.2),  # As in paper\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract features from backbone\n",
    "        x = self.backbone.features(x)\n",
    "        # Global average pooling\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, 1)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Classification head\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da177a",
   "metadata": {},
   "source": [
    "## CELL 7: Model Architecture - VGG-16 (BEST MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53cd6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    VGG-16 fine-tuned for violence detection (BEST PERFORMER).\n",
    "    \n",
    "    CRITICAL HYPERPARAMETERS (from paper experiments):\n",
    "    - FC layer size: 500 units (NOT 2048 or 1000)\n",
    "      * 2048 units -> 15% accuracy (SEVERE OVERFITTING)\n",
    "      * 1000 units -> 69% accuracy\n",
    "      * 500 units -> 71% TEST ACCURACY âœ“\n",
    "    \n",
    "    - Learning rate: 0.0001 (NOT default 0.001)\n",
    "      * 0.001 -> Model diverges (15% accuracy)\n",
    "      * 0.0001 -> 71% test accuracy âœ“\n",
    "    \n",
    "    - Dropout: 0.1 in head\n",
    "    \n",
    "    Paper result: 71% test accuracy (SOTA for this dataset)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes: int = 8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained VGG-16\n",
    "        self.backbone = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove original classifier\n",
    "        self.backbone.classifier = nn.Sequential()\n",
    "        \n",
    "        # Get feature dimension (25088 after flattening)\n",
    "        num_features = 25088\n",
    "        \n",
    "        # Custom classifier head with the critical FC=500\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, 500),  # CRITICAL: 500, not 2048!\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),  # As in paper\n",
    "            nn.Linear(500, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract features using VGG backbone\n",
    "        x = self.backbone.features(x)\n",
    "        # Flatten\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Classification head\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a23401",
   "metadata": {},
   "source": [
    "## CELL 8: Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e100343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, \n",
    "                train_loader: DataLoader,\n",
    "                criterion: nn.Module,\n",
    "                optimizer: optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                model_name: str = \"model\") -> Tuple[float, float]:\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"[{model_name}] Training\")\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model: nn.Module,\n",
    "             val_loader: DataLoader,\n",
    "             criterion: nn.Module,\n",
    "             device: torch.device,\n",
    "             model_name: str = \"model\") -> Tuple[float, float]:\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=f\"[{model_name}] Validating\")\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = total_loss / len(val_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                model_name: str,\n",
    "                hyperparams: Dict,\n",
    "                num_epochs: int = 50,\n",
    "                device: torch.device = None) -> Dict:\n",
    "    \"\"\"Complete training loop with early stopping.\"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=hyperparams['learning_rate'],\n",
    "        betas=CONFIG['adam_betas']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=CONFIG['lr_scheduler_factor'],\n",
    "        patience=CONFIG['lr_scheduler_patience'],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'best_epoch': 0,\n",
    "        'best_val_acc': 0.0\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    patience_counter = 0\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Learning Rate: {hyperparams['learning_rate']}\")\n",
    "    print(f\"Batch Size: {CONFIG['batch_size_per_gpu']}\")\n",
    "    print(f\"Epochs: {num_epochs}\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, model_name\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate(\n",
    "            model, val_loader, criterion, device, model_name\n",
    "        )\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            history['best_epoch'] = epoch + 1\n",
    "            history['best_val_acc'] = val_acc\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = f\"{CONFIG['models_dir']}/{model_name}_best.pt\"\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"âœ“ Best model saved at epoch {epoch+1}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= CONFIG['early_stopping_patience']:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = f\"{CONFIG['models_dir']}/{model_name}_best.pt\"\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c994ef",
   "metadata": {},
   "source": [
    "# CELL 9: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32c59350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module,\n",
    "                   test_loader: DataLoader,\n",
    "                   device: torch.device,\n",
    "                   model_name: str) -> Dict:\n",
    "    \"\"\"Comprehensive evaluation on test set.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # One-vs-rest AUC\n",
    "    auc_scores = []\n",
    "    for i in range(CONFIG['num_classes']):\n",
    "        try:\n",
    "            auc = roc_auc_score((all_labels == i).astype(int), all_probs[:, i])\n",
    "            auc_scores.append(auc)\n",
    "        except:\n",
    "            pass\n",
    "    mean_auc = np.mean(auc_scores) if auc_scores else 0.0\n",
    "    \n",
    "    # Top-K accuracy\n",
    "    top_k_acc = {}\n",
    "    for k in CONFIG['top_k_values']:\n",
    "        try:\n",
    "            top_k_acc[f'top_{k}'] = top_k_accuracy_score(all_labels, all_probs, k=k)\n",
    "        except:\n",
    "            top_k_acc[f'top_{k}'] = 0.0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                  target_names=[CONFIG['classes'][i] for i in range(CONFIG['num_classes'])])\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'mean_auc': mean_auc,\n",
    "        'auc_per_class': auc_scores,\n",
    "        'top_k_accuracy': top_k_acc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} - Test Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Mean AUC: {mean_auc:.4f}\")\n",
    "    for k, v in top_k_acc.items():\n",
    "        print(f\"  {k} Accuracy: {v:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3905f2",
   "metadata": {},
   "source": [
    "# CELL 10: Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc545287",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train Sequential CNN\u001b[39;00m\n\u001b[1;32m      5\u001b[0m scnn \u001b[38;5;241m=\u001b[39m SequentialCNN(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m scnn_history, scnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSequential_CNN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscnn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluate S-CNN\u001b[39;00m\n\u001b[1;32m     17\u001b[0m scnn_metrics \u001b[38;5;241m=\u001b[39m evaluate_model(scnn_model, test_loader, device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequential_CNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 101\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, model_name, hyperparams, num_epochs, device)\u001b[0m\n\u001b[1;32m     94\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m     95\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     96\u001b[0m     lr\u001b[38;5;241m=\u001b[39mhyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     97\u001b[0m     betas\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_betas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Learning rate scheduler\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr_scheduler_factor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr_scheduler_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Training history\u001b[39;00m\n\u001b[1;32m    110\u001b[0m history \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_val_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    117\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Train Sequential CNN\n",
    "scnn = SequentialCNN(CONFIG['num_classes'])\n",
    "scnn_history, scnn_model = train_model(\n",
    "    scnn,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    \"Sequential_CNN\",\n",
    "    CONFIG['scnn'],\n",
    "    num_epochs=CONFIG['epochs'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Evaluate S-CNN\n",
    "scnn_metrics = evaluate_model(scnn_model, test_loader, device, \"Sequential_CNN\")\n",
    "\n",
    "# Train MobileNetV2\n",
    "mobilenet = MobileNetV2Classifier(CONFIG['num_classes'])\n",
    "mobilenet_history, mobilenet_model = train_model(\n",
    "    mobilenet,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    \"MobileNetV2\",\n",
    "    CONFIG['mobilenetv2'],\n",
    "    num_epochs=CONFIG['epochs'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Evaluate MobileNetV2\n",
    "mobilenet_metrics = evaluate_model(mobilenet_model, test_loader, device, \"MobileNetV2\")\n",
    "\n",
    "# Train VGG-16 (CRITICAL: Use specific hyperparameters)\n",
    "vgg16 = VGG16Classifier(CONFIG['num_classes'])\n",
    "vgg16_history, vgg16_model = train_model(\n",
    "    vgg16,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    \"VGG16\",\n",
    "    CONFIG['vgg16'],\n",
    "    num_epochs=CONFIG['epochs'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Evaluate VGG-16\n",
    "vgg16_metrics = evaluate_model(vgg16_model, test_loader, device, \"VGG-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bda679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "major project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
