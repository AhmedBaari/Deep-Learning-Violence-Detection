{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ecc7c7",
   "metadata": {},
   "source": [
    "## CELL 1: Imports & DDP Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eee96c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NOTEBOOK 4: DISTRIBUTED TRAINING PIPELINE\n",
      "================================================================================\n",
      "PyTorch Version: 2.9.1+cu128\n",
      "CUDA Available: True\n",
      "✓ MODE: SINGLE-GPU (Jupyter) - Testing/Development\n",
      "  Device: cuda:0\n",
      "  NOTE: For production training with 8 GPUs, use:\n",
      "        torchrun --nproc_per_node=8 training_script.py\n",
      "\n",
      "GPU Information:\n",
      "  GPU 0: NVIDIA H200 - 150.1GB\n",
      "  GPU 1: NVIDIA H200 - 150.1GB\n",
      "  GPU 2: NVIDIA H200 - 150.1GB\n",
      "  GPU 3: NVIDIA H200 - 150.1GB\n",
      "  GPU 4: NVIDIA H200 - 150.1GB\n",
      "  GPU 5: NVIDIA H200 - 150.1GB\n",
      "  GPU 6: NVIDIA H200 - 150.1GB\n",
      "  GPU 7: NVIDIA H200 - 150.1GB\n",
      "================================================================================\n",
      "\n",
      "✓ Random seeds set (base=42, rank offset=0)\n",
      "✓ Initialization complete\n",
      "✓ Mode: Single-GPU\n",
      "✓ Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: IMPORTS & DDP INITIALIZATION\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Imports all required libraries\n",
    "2. Initializes PyTorch Distributed Data Parallel (DDP) OR single-GPU mode\n",
    "3. Verifies GPUs are available\n",
    "4. Sets up process rank and world size\n",
    "\n",
    "MODES:\n",
    "- DDP Mode: Launch with `torchrun --nproc_per_node=8 script.py`\n",
    "- Single-GPU Mode: Run directly in Jupyter (for testing)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Vision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Timm for ViT\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    print(\"Installing timm library...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"timm\"])\n",
    "    import timm\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZE DISTRIBUTED TRAINING (OR SINGLE-GPU MODE)\n",
    "# ============================================================\n",
    "\n",
    "def setup_ddp():\n",
    "    \"\"\"\n",
    "    Initialize PyTorch Distributed Data Parallel OR single-GPU mode.\n",
    "    \n",
    "    Automatically detects if running with torchrun (DDP) or Jupyter (single-GPU).\n",
    "    \n",
    "    Returns:\n",
    "        rank: GPU rank (0 for single-GPU, 0-7 for DDP)\n",
    "        world_size: Total number of GPUs (1 for single-GPU, 8 for DDP)\n",
    "        use_ddp: Boolean indicating if DDP is active\n",
    "    \"\"\"\n",
    "    # Check if DDP environment variables are set (torchrun sets these)\n",
    "    use_ddp = 'RANK' in os.environ and 'WORLD_SIZE' in os.environ\n",
    "    \n",
    "    if use_ddp:\n",
    "        # DDP mode (launched with torchrun)\n",
    "        dist.init_process_group(backend='nccl')\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        torch.cuda.set_device(rank)\n",
    "    else:\n",
    "        # Single-GPU mode (Jupyter notebook)\n",
    "        rank = 0\n",
    "        world_size = 1\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(0)\n",
    "    \n",
    "    return rank, world_size, use_ddp\n",
    "\n",
    "# Initialize (auto-detect mode)\n",
    "rank, world_size, use_ddp = setup_ddp()\n",
    "device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Print info only on rank 0 (master process)\n",
    "if rank == 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"NOTEBOOK 4: DISTRIBUTED TRAINING PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if use_ddp:\n",
    "        print(f\"✓ MODE: DISTRIBUTED (DDP) - Multi-GPU Training\")\n",
    "        print(f\"  NCCL Backend: {dist.is_nccl_available()}\")\n",
    "        print(f\"  World Size: {world_size} GPUs\")\n",
    "        print(f\"  Backend: NCCL\")\n",
    "        \n",
    "        # Verify 8 GPUs for production training\n",
    "        if world_size != 8:\n",
    "            print(f\"  ⚠ WARNING: Expected 8 GPUs for production, got {world_size}\")\n",
    "        else:\n",
    "            print(f\"  ✓ All 8 H200 GPUs detected\")\n",
    "    else:\n",
    "        print(f\"✓ MODE: SINGLE-GPU (Jupyter) - Testing/Development\")\n",
    "        print(f\"  Device: {device}\")\n",
    "        print(f\"  NOTE: For production training with 8 GPUs, use:\")\n",
    "        print(f\"        torchrun --nproc_per_node=8 training_script.py\")\n",
    "    \n",
    "    # Print GPU info\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"\\nGPU Information:\")\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        for i in range(num_gpus):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            print(f\"  GPU {i}: {props.name} - {props.total_memory / 1e9:.1f}GB\")\n",
    "    else:\n",
    "        print(\"\\n⚠ WARNING: No CUDA GPUs detected! Training will be very slow.\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42 + rank  # Different seed per GPU for augmentation diversity\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"\\n✓ Random seeds set (base=42, rank offset={rank})\")\n",
    "    print(f\"✓ Initialization complete\")\n",
    "    print(f\"✓ Mode: {'DDP' if use_ddp else 'Single-GPU'}\")\n",
    "    print(f\"✓ Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57fe75",
   "metadata": {},
   "source": [
    "## CELL 2: Load Configuration & Data Splits\n",
    "\n",
    "Load saved data from Notebook 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25686b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING CONFIGURATION & DATA\n",
      "================================================================================\n",
      "✓ Loaded configuration from novelty_files/configs/notebook_01_config.json\n",
      "✓ Loaded class mappings (8 classes)\n",
      "✓ Loaded splits:\n",
      "  Train: 53,097 samples\n",
      "  Val:   11,379 samples\n",
      "  Test:  11,379 samples\n",
      "✓ Reloaded 75,855 samples\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD CONFIGURATION & DATA SPLITS\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Load all necessary data from Notebook 1:\n",
    "- Configuration\n",
    "- Train/val/test indices\n",
    "- Class mappings\n",
    "- Dataset samples\n",
    "\"\"\"\n",
    "\n",
    "base_dir = Path('./novelty_files')\n",
    "\n",
    "# ============================================================\n",
    "# LOAD CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING CONFIGURATION & DATA\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "config_path = base_dir / 'configs' / 'notebook_01_config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"✓ Loaded configuration from {config_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD CLASS DISTRIBUTION\n",
    "# ============================================================\n",
    "\n",
    "dist_path = base_dir / 'splits' / 'class_distribution.json'\n",
    "with open(dist_path, 'r') as f:\n",
    "    dist_data = json.load(f)\n",
    "\n",
    "class_to_idx = dist_data['class_to_idx']\n",
    "idx_to_class = {int(k): v for k, v in dist_data['idx_to_class'].items()}\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"✓ Loaded class mappings ({len(class_to_idx)} classes)\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD TRAIN/VAL/TEST SPLITS\n",
    "# ============================================================\n",
    "\n",
    "with open(base_dir / 'splits' / 'train_indices.pkl', 'rb') as f:\n",
    "    train_indices = pickle.load(f)\n",
    "with open(base_dir / 'splits' / 'val_indices.pkl', 'rb') as f:\n",
    "    val_indices = pickle.load(f)\n",
    "with open(base_dir / 'splits' / 'test_indices.pkl', 'rb') as f:\n",
    "    test_indices = pickle.load(f)\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"✓ Loaded splits:\")\n",
    "    print(f\"  Train: {len(train_indices):,} samples\")\n",
    "    print(f\"  Val:   {len(val_indices):,} samples\")\n",
    "    print(f\"  Test:  {len(test_indices):,} samples\")\n",
    "\n",
    "# ============================================================\n",
    "# RELOAD DATASET SAMPLES\n",
    "# ============================================================\n",
    "\n",
    "class HMDB51FightDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, split: str, class_to_idx: Dict[str, int]):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = []\n",
    "        \n",
    "        split_dir = self.root_dir / split\n",
    "        for class_name, class_idx in class_to_idx.items():\n",
    "            class_dir = split_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png'))\n",
    "                for img_path in image_files:\n",
    "                    self.samples.append({\n",
    "                        'path': str(img_path),\n",
    "                        'label': class_idx,\n",
    "                        'class_name': class_name\n",
    "                    })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "dataset_path = CONFIG['dataset_path']\n",
    "train_dataset_loader = HMDB51FightDataset(dataset_path, 'train', class_to_idx)\n",
    "test_dataset_loader = HMDB51FightDataset(dataset_path, 'test', class_to_idx)\n",
    "\n",
    "all_samples = train_dataset_loader.samples + test_dataset_loader.samples\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"✓ Reloaded {len(all_samples):,} samples\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f55431",
   "metadata": {},
   "source": [
    "## CELL 3: MixUp & CutMix Augmentation Functions\n",
    "\n",
    "Implement advanced augmentation techniques for improved generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7972f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AUGMENTATION FUNCTIONS DEFINED\n",
      "================================================================================\n",
      "✓ MixUp: Linear interpolation between images\n",
      "✓ CutMix: Cut-and-paste patch augmentation\n",
      "✓ Mixed criterion: Weighted loss combination\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: MIXUP & CUTMIX AUGMENTATION\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Implement MixUp and CutMix augmentation for improved generalization.\n",
    "\n",
    "MixUp: Linear interpolation between two images\n",
    "  mixed_img = λ * img1 + (1-λ) * img2\n",
    "  mixed_label = λ * label1 + (1-λ) * label2\n",
    "\n",
    "CutMix: Cut and paste patches between images\n",
    "  Cut rectangular patch from img2, paste into img1\n",
    "  Label mixing based on patch area ratio\n",
    "\"\"\"\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply MixUp augmentation.\n",
    "    \n",
    "    Args:\n",
    "        x: Input images tensor (batch_size, 3, 224, 224)\n",
    "        y: Labels tensor (batch_size,)\n",
    "        alpha: MixUp interpolation strength (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        mixed_x: Mixed images\n",
    "        y_a, y_b: Original labels\n",
    "        lam: Mixing coefficient\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply CutMix augmentation.\n",
    "    \n",
    "    Args:\n",
    "        x: Input images tensor (batch_size, 3, 224, 224)\n",
    "        y: Labels tensor (batch_size,)\n",
    "        alpha: CutMix interpolation strength (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        mixed_x: Mixed images with cutout patches\n",
    "        y_a, y_b: Original labels\n",
    "        lam: Mixing coefficient (based on cut area)\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    _, _, H, W = x.shape\n",
    "    \n",
    "    # Calculate cut dimensions\n",
    "    cut_rat = np.sqrt(1.0 - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    # Uniform random position for cut\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    # Calculate bounding box\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    # Apply CutMix\n",
    "    mixed_x = x.clone()\n",
    "    mixed_x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    # Adjust lambda based on actual cut area\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"\n",
    "    Compute mixed loss for MixUp/CutMix.\n",
    "    \n",
    "    Args:\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss)\n",
    "        pred: Model predictions\n",
    "        y_a, y_b: Original labels\n",
    "        lam: Mixing coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Mixed loss\n",
    "    \"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AUGMENTATION FUNCTIONS DEFINED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"✓ MixUp: Linear interpolation between images\")\n",
    "    print(\"✓ CutMix: Cut-and-paste patch augmentation\")\n",
    "    print(\"✓ Mixed criterion: Weighted loss combination\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a627ba4",
   "metadata": {},
   "source": [
    "## CELL 4: Create Distributed DataLoaders\n",
    "\n",
    "Create DataLoaders with DistributedSampler for multi-GPU training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6544a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DISTRIBUTED DATALOADERS CREATED\n",
      "================================================================================\n",
      "Batch Size per GPU: 64\n",
      "Number of GPUs: 1\n",
      "Gradient Accumulation Steps: 4\n",
      "Effective Batch Size: 256 = 64×1×4\n",
      "\n",
      "Train: 829 batches/GPU × 1 GPUs\n",
      "Val:   178 batches/GPU × 1 GPUs\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: CREATE DISTRIBUTED DATALOADERS\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Create DataLoaders with DistributedSampler.\n",
    "\n",
    "Each GPU gets a unique subset of data via DistributedSampler.\n",
    "Batch size per GPU: 64\n",
    "Total effective batch: 64 × 8 GPUs × 4 accumulation = 2048\n",
    "\"\"\"\n",
    "\n",
    "class HMDB51Dataset(Dataset):\n",
    "    def __init__(self, samples, indices, transform=None):\n",
    "        self.samples = [samples[i] for i in indices]\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        img = Image.open(sample['path']).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, sample['label']\n",
    "\n",
    "# Define transforms\n",
    "vit_mean = [0.485, 0.456, 0.406]\n",
    "vit_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=vit_mean, std=vit_std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=vit_mean, std=vit_std),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HMDB51Dataset(all_samples, train_indices, transform=train_transform)\n",
    "val_dataset = HMDB51Dataset(all_samples, val_indices, transform=val_transform)\n",
    "\n",
    "# Create DistributedSampler (critical for DDP!)\n",
    "train_sampler = DistributedSampler(\n",
    "    train_dataset,\n",
    "    num_replicas=world_size,\n",
    "    rank=rank,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_sampler = DistributedSampler(\n",
    "    val_dataset,\n",
    "    num_replicas=world_size,\n",
    "    rank=rank,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64  # Per GPU\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=val_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DISTRIBUTED DATALOADERS CREATED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Batch Size per GPU: {BATCH_SIZE}\")\n",
    "    print(f\"Number of GPUs: {world_size}\")\n",
    "    print(f\"Gradient Accumulation Steps: 4\")\n",
    "    print(f\"Effective Batch Size: {BATCH_SIZE * world_size * 4} = {BATCH_SIZE}×{world_size}×4\")\n",
    "    print(f\"\\nTrain: {len(train_loader)} batches/GPU × {world_size} GPUs\")\n",
    "    print(f\"Val:   {len(val_loader)} batches/GPU × {world_size} GPUs\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e922a8",
   "metadata": {},
   "source": [
    "## CELL 5: Load ViT Model & Wrap in DDP\n",
    "\n",
    "Load pretrained ViT from Notebook 2 and wrap in DistributedDataParallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed3367c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ViT baseline from: novelty_files/checkpoints/vit_baseline.pt\n",
      "✓ Loaded checkpoint (val_acc=97.39%)\n",
      "✓ Model running in single-GPU mode\n",
      "\n",
      "================================================================================\n",
      "MODEL SETUP COMPLETE\n",
      "================================================================================\n",
      "Model: ViT-Base/16\n",
      "Parameters: 85.8M\n",
      "Mode: Single-GPU\n",
      "Device: cuda:0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: LOAD MODEL & WRAP IN DDP (IF ACTIVE)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Load ViT-Base model and wrap in DistributedDataParallel (if using DDP).\n",
    "\n",
    "Options:\n",
    "1. Load from Notebook 2 baseline checkpoint (if exists)\n",
    "2. Load fresh ViT-Base from timm\n",
    "\n",
    "DDP wrapping only happens if launched with torchrun.\n",
    "\"\"\"\n",
    "\n",
    "# Load ViT-Base model\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=8)\n",
    "\n",
    "# Try to load Notebook 2 checkpoint if it exists\n",
    "vit_baseline_path = base_dir / 'checkpoints' / 'vit_baseline.pt'\n",
    "if vit_baseline_path.exists():\n",
    "    if rank == 0:\n",
    "        print(f\"\\nLoading ViT baseline from: {vit_baseline_path}\")\n",
    "    checkpoint = torch.load(vit_baseline_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if rank == 0:\n",
    "        print(f\"✓ Loaded checkpoint (val_acc={checkpoint['val_accuracy']:.2f}%)\")\n",
    "else:\n",
    "    if rank == 0:\n",
    "        print(\"\\nNo baseline checkpoint found, using ImageNet pretrained weights\")\n",
    "\n",
    "# Move to device BEFORE wrapping in DDP\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap in DDP ONLY if running in distributed mode\n",
    "if use_ddp:\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=False)\n",
    "    if rank == 0:\n",
    "        print(\"✓ Model wrapped in DistributedDataParallel\")\n",
    "else:\n",
    "    if rank == 0:\n",
    "        print(\"✓ Model running in single-GPU mode\")\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL SETUP COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    # Access model correctly (DDP wraps the model in .module)\n",
    "    param_model = model.module if use_ddp else model\n",
    "    total_params = sum(p.numel() for p in param_model.parameters())\n",
    "    print(f\"Model: ViT-Base/16\")\n",
    "    print(f\"Parameters: {total_params/1e6:.1f}M\")\n",
    "    print(f\"Mode: {'DDP (multi-GPU)' if use_ddp else 'Single-GPU'}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83f09a",
   "metadata": {},
   "source": [
    "## CELL 6: Main DDP Training Loop (Resume-Safe)\n",
    "\n",
    "Full training loop with gradient accumulation, MixUp/CutMix, and checkpointing.  \n",
    "**This is the core training cell - runs for ~3 hours on 8 GPUs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c7e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING DISTRIBUTED TRAINING\n",
      "================================================================================\n",
      "Epochs: 50\n",
      "Learning Rate: 0.0001\n",
      "Batch Size (per GPU): 64\n",
      "Gradient Accumulation: 4 steps\n",
      "Effective Batch Size: 256\n",
      "MixUp Probability: 0.5\n",
      "CutMix Probability: 0.5\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/50 \n",
      "------------------------------------------------------------\n",
      "  Batch 100/829: Loss=0.1072\n",
      "  Batch 200/829: Loss=0.9207\n",
      "  Batch 300/829: Loss=1.0299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m GRAD_ACCUM_STEPS\n\u001b[1;32m    105\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    106\u001b[0m train_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: MAIN DDP TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Full distributed training with:\n",
    "- Gradient accumulation (4 steps)\n",
    "- MixUp/CutMix (50% probability each)\n",
    "- Cosine annealing LR schedule\n",
    "- Gradient clipping\n",
    "- Checkpoint save/resume\n",
    "\"\"\"\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "MAX_GRAD_NORM = 1.0\n",
    "MIXUP_PROB = 0.5\n",
    "CUTMIX_PROB = 0.5\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Check for existing checkpoint (resume-safe)\n",
    "best_checkpoint_path = base_dir / 'checkpoints' / 'ddp_best_model.pt'\n",
    "start_epoch = 0\n",
    "best_val_acc = 0.0\n",
    "\n",
    "if best_checkpoint_path.exists() and rank == 0:\n",
    "    print(  f\"\\nFound existing checkpoint: {best_checkpoint_path}\")\n",
    "    print(\"To resume training, load checkpoint here.\")\n",
    "    # checkpoint = torch.load(best_checkpoint_path)\n",
    "    # model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    # start_epoch = checkpoint['epoch']\n",
    "    # best_val_acc = checkpoint['best_val_acc']\n",
    "\n",
    "if rank == 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING DISTRIBUTED TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"Batch Size (per GPU): {BATCH_SIZE}\")\n",
    "    print(f\"Gradient Accumulation: {GRAD_ACCUM_STEPS} steps\")\n",
    "    print(f\"Effective Batch Size: {BATCH_SIZE * world_size * GRAD_ACCUM_STEPS}\")\n",
    "    print(f\"MixUp Probability: {MIXUP_PROB}\")\n",
    "    print(f\"CutMix Probability: {CUTMIX_PROB}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    # Set epoch for distributed sampler (important!)\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    \n",
    "    # TRAINING PHASE\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} \")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Apply augmentation randomly\n",
    "        use_mixup = np.random.random() < MIXUP_PROB\n",
    "        use_cutmix = np.random.random() < CUTMIX_PROB and not use_mixup\n",
    "        \n",
    "        if use_mixup:\n",
    "            images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=1.0)\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        elif use_cutmix:\n",
    "            images, labels_a, labels_b, lam = cutmix_data(images, labels, alpha=1.0)\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / GRAD_ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print progress (rank 0 only, every 100 batches)\n",
    "        if rank == 0 and (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)}: Loss={loss.item()*GRAD_ACCUM_STEPS:.4f}\")\n",
    "    \n",
    "    # Compute training metrics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100.0 * train_correct / train_total\n",
    "    \n",
    "    # Gather metrics from all GPUs\n",
    "    train_loss_tensor = torch.tensor([avg_train_loss], device=device)\n",
    "    train_acc_tensor = torch.tensor([train_acc], device=device)\n",
    "    dist.all_reduce(train_loss_tensor, op=dist.ReduceOp.SUM)\n",
    "    dist.all_reduce(train_acc_tensor, op=dist.ReduceOp.SUM)\n",
    "    avg_train_loss = (train_loss_tensor / world_size).item()\n",
    "    train_acc = (train_acc_tensor / world_size).item()\n",
    "    \n",
    "    # VALIDATION PHASE (every epoch)\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # Gather validation metrics\n",
    "    val_correct_tensor = torch.tensor([val_correct], device=device)\n",
    "    val_total_tensor = torch.tensor([val_total], device=device)\n",
    "    dist.all_reduce(val_correct_tensor, op=dist.ReduceOp.SUM)\n",
    "    dist.all_reduce(val_total_tensor, op=dist.ReduceOp.SUM)\n",
    "    val_acc = 100.0 * val_correct_tensor.item() / val_total_tensor.item()\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print metrics (rank 0 only)\n",
    "    if rank == 0:\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save checkpoint if best\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.module.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'config': {\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'learning_rate': LEARNING_RATE,\n",
    "                    'num_epochs': NUM_EPOCHS,\n",
    "                }\n",
    "            }\n",
    "            torch.save(checkpoint, best_checkpoint_path)\n",
    "            print(f\"  ✓ Best model saved! (val_acc={val_acc:.2f}%)\")\n",
    "        \n",
    "        # Save periodic checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            periodic_path = base_dir / 'checkpoints' / f'ddp_epoch_{epoch+1}.pt'\n",
    "            torch.save(checkpoint, periodic_path)\n",
    "            print(f\"  ✓ Periodic checkpoint saved: {periodic_path.name}\")\n",
    "\n",
    "# Cleanup\n",
    "if rank == 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Expected: 80-85% (ViT + Augmentation)\")\n",
    "    print(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "dist.destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "major project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
