{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63522f89",
   "metadata": {},
   "source": [
    "## CELL 1: Environment Setup & Load Previous Results\n",
    "\n",
    "Load configuration and best model from DDP training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c08fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NOTEBOOK 6: ADVERSARIAL FINE-TUNING & CBAM ATTENTION\n",
      "================================================================================\n",
      "PyTorch Version: 2.9.1+cu128\n",
      "CUDA Available: True\n",
      "Device Count: 8\n",
      "Active GPU: NVIDIA H200\n",
      "================================================================================\n",
      "\n",
      "✓ Using device: cuda:0\n",
      "✓ Random seeds set to 42\n",
      "\n",
      "✓ Loaded configuration from novelty_files/configs/notebook_01_config.json\n",
      "✓ Loaded class mappings (8 classes)\n",
      "✓ Loaded splits: 53,097 train, 11,379 val, 11,379 test\n",
      "\n",
      "================================================================================\n",
      "INITIALIZATION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: ENVIRONMENT SETUP & IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Imports all required libraries\n",
    "2. Loads configuration from Notebook 1\n",
    "3. Loads best model from Notebook 4 (DDP training)\n",
    "4. Sets up device and random seeds\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Vision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Data & ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# timm for ViT\n",
    "import timm\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NOTEBOOK 6: ADVERSARIAL FINE-TUNING & CBAM ATTENTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Active GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n✓ Using device: {device}\")\n",
    "\n",
    "# Set random seeds\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(f\"✓ Random seeds set to {RANDOM_SEED}\")\n",
    "\n",
    "# ============================================================\n",
    "# LOAD CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "base_dir = Path('./novelty_files')\n",
    "config_path = base_dir / 'configs' / 'notebook_01_config.json'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "print(f\"\\n✓ Loaded configuration from {config_path}\")\n",
    "\n",
    "# Load class mappings\n",
    "dist_path = base_dir / 'splits' / 'class_distribution.json'\n",
    "with open(dist_path, 'r') as f:\n",
    "    dist_data = json.load(f)\n",
    "\n",
    "class_to_idx = dist_data['class_to_idx']\n",
    "idx_to_class = {int(k): v for k, v in dist_data['idx_to_class'].items()}\n",
    "\n",
    "print(f\"✓ Loaded class mappings ({len(class_to_idx)} classes)\")\n",
    "\n",
    "# Load splits\n",
    "with open(base_dir / 'splits' / 'train_indices.pkl', 'rb') as f:\n",
    "    train_indices = pickle.load(f)\n",
    "with open(base_dir / 'splits' / 'val_indices.pkl', 'rb') as f:\n",
    "    val_indices = pickle.load(f)\n",
    "with open(base_dir / 'splits' / 'test_indices.pkl', 'rb') as f:\n",
    "    test_indices = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded splits: {len(train_indices):,} train, {len(val_indices):,} val, {len(test_indices):,} test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INITIALIZATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551066a5",
   "metadata": {},
   "source": [
    "## CELL 2: Implement CBAM (Convolutional Block Attention Module)\n",
    "\n",
    "CBAM applies both channel attention and spatial attention to feature maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bc95c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IMPLEMENTING CBAM ATTENTION\n",
      "================================================================================\n",
      "✓ CBAM modules implemented:\n",
      "  • ChannelAttention - Focus on important channels\n",
      "  • SpatialAttention - Focus on important regions\n",
      "  • CBAM - Combined channel + spatial attention\n",
      "\n",
      "Testing CBAM module...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input shape:  torch.Size([2, 768, 14, 14])\n",
      "  Output shape: torch.Size([2, 768, 14, 14])\n",
      "  ✓ CBAM forward pass successful\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: CBAM ATTENTION MODULE\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "CBAM (Convolutional Block Attention Module):\n",
    "- Channel Attention: Learns WHAT to focus on\n",
    "- Spatial Attention: Learns WHERE to focus\n",
    "\n",
    "Paper: \"CBAM: Convolutional Block Attention Module\" (ECCV 2018)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTING CBAM ATTENTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# CHANNEL ATTENTION MODULE\n",
    "# ============================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Channel Attention Module.\n",
    "    \n",
    "    Applies attention across feature channels using both\n",
    "    max pooling and average pooling, then combines them.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels\n",
    "        reduction: Channel reduction ratio (default: 16)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        \n",
    "        # Shared MLP for both pooling operations\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        )\n",
    "        \n",
    "        # Sigmoid for final attention weights\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, channels, height, width)\n",
    "        Returns:\n",
    "            Attended features (batch, channels, height, width)\n",
    "        \"\"\"\n",
    "        # Global average pooling: (B, C, H, W) -> (B, C)\n",
    "        avg_pool = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)\n",
    "        \n",
    "        # Global max pooling: (B, C, H, W) -> (B, C)\n",
    "        max_pool = F.adaptive_max_pool2d(x, 1).view(x.size(0), -1)\n",
    "        \n",
    "        # Apply shared MLP\n",
    "        avg_out = self.mlp(avg_pool)\n",
    "        max_out = self.mlp(max_pool)\n",
    "        \n",
    "        # Combine and apply sigmoid\n",
    "        channel_attention = self.sigmoid(avg_out + max_out)\n",
    "        \n",
    "        # Reshape and multiply: (B, C) -> (B, C, 1, 1)\n",
    "        channel_attention = channel_attention.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        return x * channel_attention\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SPATIAL ATTENTION MODULE\n",
    "# ============================================================\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatial Attention Module.\n",
    "    \n",
    "    Applies attention across spatial locations using both\n",
    "    max and average pooling across channels.\n",
    "    \n",
    "    Args:\n",
    "        kernel_size: Convolution kernel size (default: 7)\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        \n",
    "        padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        # Convolutional layer to generate spatial attention map\n",
    "        self.conv = nn.Conv2d(\n",
    "            2,  # Input: concatenated avg and max pooling\n",
    "            1,  # Output: single attention map\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, channels, height, width)\n",
    "        Returns:\n",
    "            Attended features (batch, channels, height, width)\n",
    "        \"\"\"\n",
    "        # Average pooling across channels: (B, C, H, W) -> (B, 1, H, W)\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        \n",
    "        # Max pooling across channels: (B, C, H, W) -> (B, 1, H, W)\n",
    "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        \n",
    "        # Concatenate: (B, 2, H, W)\n",
    "        concat = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        \n",
    "        # Apply convolution and sigmoid\n",
    "        spatial_attention = self.sigmoid(self.conv(concat))\n",
    "        \n",
    "        return x * spatial_attention\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CBAM MODULE (CHANNEL + SPATIAL)\n",
    "# ============================================================\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete CBAM module combining channel and spatial attention.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels\n",
    "        reduction: Channel reduction ratio for channel attention\n",
    "        kernel_size: Kernel size for spatial attention\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        \n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, channels, height, width)\n",
    "        Returns:\n",
    "            Attended features (batch, channels, height, width)\n",
    "        \"\"\"\n",
    "        # Apply channel attention first\n",
    "        x = self.channel_attention(x)\n",
    "        \n",
    "        # Then apply spatial attention\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ CBAM modules implemented:\")\n",
    "print(\"  • ChannelAttention - Focus on important channels\")\n",
    "print(\"  • SpatialAttention - Focus on important regions\")\n",
    "print(\"  • CBAM - Combined channel + spatial attention\")\n",
    "\n",
    "# Test CBAM\n",
    "print(\"\\nTesting CBAM module...\")\n",
    "test_input = torch.randn(2, 768, 14, 14).to(device)  # ViT feature maps\n",
    "cbam = CBAM(in_channels=768).to(device)\n",
    "test_output = cbam(test_input)\n",
    "\n",
    "print(f\"  Input shape:  {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  ✓ CBAM forward pass successful\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6711ade5",
   "metadata": {},
   "source": [
    "## CELL 3: Load Best Model & Integrate CBAM\n",
    "\n",
    "Load the best model from Notebook 4 and add CBAM attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddefe6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING BEST MODEL & INTEGRATING CBAM\n",
      "================================================================================\n",
      "Loading model from: novelty_files/checkpoints/ddp_best_model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded pretrained weights\n",
      "✓ Created ViT + CBAM model\n",
      "  Total parameters: 85.9M\n",
      "  Trainable parameters: 85.9M\n",
      "\n",
      "Testing forward pass...\n",
      "  Input shape:  torch.Size([2, 3, 224, 224])\n",
      "  Output shape: torch.Size([2, 8])\n",
      "  ✓ Forward pass successful\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: LOAD BEST MODEL & INTEGRATE CBAM\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This cell:\n",
    "1. Loads the best model from Notebook 4 (DDP training)\n",
    "2. Creates a new model with CBAM attention integrated\n",
    "3. Transfers weights from the pretrained model\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING BEST MODEL & INTEGRATING CBAM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# FIND BEST CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "checkpoint_dir = base_dir / 'checkpoints'\n",
    "\n",
    "# Check for DDP best model first\n",
    "best_model_path = checkpoint_dir / 'ddp_best_model.pt'\n",
    "\n",
    "if not best_model_path.exists():\n",
    "    # Fallback to ViT baseline\n",
    "    best_model_path = checkpoint_dir / 'vit_baseline.pt'\n",
    "    print(f\"⚠ DDP model not found, using ViT baseline instead\")\n",
    "\n",
    "if not best_model_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"No trained model found!\\n\"\n",
    "        f\"Please run Notebook 4 (DDP training) first.\"\n",
    "    )\n",
    "\n",
    "print(f\"Loading model from: {best_model_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE BASE VIT MODEL\n",
    "# ============================================================\n",
    "\n",
    "# Create ViT-Base model\n",
    "base_model = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=8)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "\n",
    "# Handle DDP state dict (remove 'module.' prefix if present)\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# Remove 'module.' prefix from DDP\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('module.'):\n",
    "        new_state_dict[k[7:]] = v  # Remove 'module.' prefix\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "base_model.load_state_dict(new_state_dict)\n",
    "print(f\"✓ Loaded pretrained weights\")\n",
    "\n",
    "if 'val_accuracy' in checkpoint:\n",
    "    print(f\"  Pretrained model accuracy: {checkpoint['val_accuracy']:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE VIT + CBAM MODEL\n",
    "# ============================================================\n",
    "\n",
    "class ViTWithCBAM(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer with CBAM attention.\n",
    "    \n",
    "    This model:\n",
    "    1. Uses ViT-Base as backbone\n",
    "    2. Reshapes patch embeddings to spatial feature maps\n",
    "    3. Applies CBAM attention\n",
    "    4. Flattens and applies classification head\n",
    "    \"\"\"\n",
    "    def __init__(self, vit_model, num_classes=8):\n",
    "        super(ViTWithCBAM, self).__init__()\n",
    "        \n",
    "        self.vit = vit_model\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # ViT-Base has 768 embedding dimension\n",
    "        self.embed_dim = 768\n",
    "        \n",
    "        # CBAM for attention\n",
    "        self.cbam = CBAM(in_channels=self.embed_dim, reduction=16)\n",
    "        \n",
    "        # New classification head (will be trained)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(self.embed_dim),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.embed_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get patch embeddings from ViT\n",
    "        x = self.vit.patch_embed(x)\n",
    "        x = self.vit._pos_embed(x)\n",
    "        x = self.vit.blocks(x)\n",
    "        x = self.vit.norm(x)\n",
    "        \n",
    "        # x shape: (batch, num_patches+1, embed_dim)\n",
    "        # num_patches = 196 for 224x224 with patch_size=16\n",
    "        \n",
    "        # Remove [CLS] token and reshape to spatial\n",
    "        patch_tokens = x[:, 1:, :]  # (batch, 196, 768)\n",
    "        batch_size = patch_tokens.shape[0]\n",
    "        \n",
    "        # Reshape to 2D feature map: (batch, 768, 14, 14)\n",
    "        h = w = 14  # sqrt(196) = 14\n",
    "        spatial_features = patch_tokens.transpose(1, 2).reshape(batch_size, self.embed_dim, h, w)\n",
    "        \n",
    "        # Apply CBAM attention\n",
    "        attended_features = self.cbam(spatial_features)\n",
    "        \n",
    "        # Global average pooling: (batch, 768, 14, 14) -> (batch, 768)\n",
    "        pooled = F.adaptive_avg_pool2d(attended_features, 1).view(batch_size, -1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model with CBAM\n",
    "model = ViTWithCBAM(base_model, num_classes=8).to(device)\n",
    "\n",
    "print(f\"✓ Created ViT + CBAM model\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"  Total parameters: {total_params/1e6:.1f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params/1e6:.1f}M\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nTesting forward pass...\")\n",
    "test_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_input)\n",
    "\n",
    "print(f\"  Input shape:  {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  ✓ Forward pass successful\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66eeac",
   "metadata": {},
   "source": [
    "## CELL 4: Implement PGD Adversarial Training\n",
    "\n",
    "PGD (Projected Gradient Descent) generates stronger adversarial examples for robust training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f4f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IMPLEMENTING PGD ADVERSARIAL TRAINING\n",
      "================================================================================\n",
      "✓ PGD adversarial training functions implemented:\n",
      "  • pgd_attack() - Generate adversarial examples\n",
      "  • train_step_with_pgd() - Train with adversarial examples\n",
      "\n",
      "PGD Parameters:\n",
      "  • Epsilon (max perturbation): 8/255 ≈ 0.031\n",
      "  • Alpha (step size): 2/255 ≈ 0.008\n",
      "  • Num steps: 7 iterations\n",
      "  • Expected: Stronger robustness than FGSM\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: PGD ADVERSARIAL TRAINING\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Projected Gradient Descent (PGD) Adversarial Training:\n",
    "- Generates adversarial examples by iteratively perturbing inputs\n",
    "- Projects perturbations to epsilon ball\n",
    "- Trains model to be robust against these adversarial examples\n",
    "\n",
    "PGD is stronger than FGSM (Fast Gradient Sign Method) because\n",
    "it uses multiple iterations to find optimal adversarial direction.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTING PGD ADVERSARIAL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def pgd_attack(model, images, labels, eps=8/255, alpha=2/255, num_steps=7):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples using PGD (Projected Gradient Descent).\n",
    "    \n",
    "    PGD is an iterative attack that:\n",
    "    1. Starts with random perturbation\n",
    "    2. Takes gradient steps to maximize loss\n",
    "    3. Projects perturbation to epsilon ball after each step\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        images: Clean input images (batch_size, 3, 224, 224)\n",
    "        labels: Ground truth labels (batch_size,)\n",
    "        eps: Maximum perturbation magnitude (L-infinity norm)\n",
    "        alpha: Step size for each iteration\n",
    "        num_steps: Number of PGD iterations\n",
    "    \n",
    "    Returns:\n",
    "        adv_images: Adversarial images with perturbations\n",
    "    \"\"\"\n",
    "    # Clone images and enable gradients\n",
    "    adv_images = images.clone().detach()\n",
    "    \n",
    "    # Start with random perturbation in [-eps, eps]\n",
    "    adv_images = adv_images + torch.empty_like(adv_images).uniform_(-eps, eps)\n",
    "    adv_images = torch.clamp(adv_images, min=0, max=1).detach()\n",
    "    \n",
    "    # PGD iterations\n",
    "    for step in range(num_steps):\n",
    "        adv_images.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(adv_images)\n",
    "        \n",
    "        # Compute loss (we want to maximize it)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get gradient sign and take step in that direction\n",
    "        grad = adv_images.grad.data\n",
    "        adv_images = adv_images.detach() + alpha * grad.sign()\n",
    "        \n",
    "        # Project back to epsilon ball around original images\n",
    "        perturbation = torch.clamp(adv_images - images, min=-eps, max=eps)\n",
    "        adv_images = torch.clamp(images + perturbation, min=0, max=1).detach()\n",
    "    \n",
    "    return adv_images\n",
    "\n",
    "\n",
    "def train_step_with_pgd(model, images, labels, optimizer, eps=8/255, alpha=2/255, num_steps=7):\n",
    "    \"\"\"\n",
    "    Single training step with PGD adversarial examples.\n",
    "    \n",
    "    This function:\n",
    "    1. Generates PGD adversarial examples\n",
    "    2. Computes loss on both clean and adversarial examples\n",
    "    3. Backpropagates and updates model\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        images: Clean input images\n",
    "        labels: Ground truth labels\n",
    "        optimizer: Optimizer (e.g., AdamW)\n",
    "        eps: PGD epsilon parameter\n",
    "        alpha: PGD step size\n",
    "        num_steps: Number of PGD iterations\n",
    "    \n",
    "    Returns:\n",
    "        loss_clean: Loss on clean examples\n",
    "        loss_adv: Loss on adversarial examples\n",
    "        loss_total: Combined loss\n",
    "        acc_clean: Accuracy on clean examples\n",
    "        acc_adv: Accuracy on adversarial examples\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass on clean images\n",
    "    outputs_clean = model(images)\n",
    "    loss_clean = F.cross_entropy(outputs_clean, labels)\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    adv_images = pgd_attack(model, images, labels, eps, alpha, num_steps)\n",
    "    \n",
    "    # Forward pass on adversarial images\n",
    "    outputs_adv = model(adv_images)\n",
    "    loss_adv = F.cross_entropy(outputs_adv, labels)\n",
    "    \n",
    "    # Combined loss (50% clean + 50% adversarial)\n",
    "    loss_total = 0.5 * loss_clean + 0.5 * loss_adv\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss_total.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute accuracies\n",
    "    _, pred_clean = outputs_clean.max(1)\n",
    "    _, pred_adv = outputs_adv.max(1)\n",
    "    \n",
    "    acc_clean = pred_clean.eq(labels).float().mean().item()\n",
    "    acc_adv = pred_adv.eq(labels).float().mean().item()\n",
    "    \n",
    "    return loss_clean.item(), loss_adv.item(), loss_total.item(), acc_clean, acc_adv\n",
    "\n",
    "\n",
    "print(\"✓ PGD adversarial training functions implemented:\")\n",
    "print(\"  • pgd_attack() - Generate adversarial examples\")\n",
    "print(\"  • train_step_with_pgd() - Train with adversarial examples\")\n",
    "\n",
    "print(\"\\nPGD Parameters:\")\n",
    "print(\"  • Epsilon (max perturbation): 8/255 ≈ 0.031\")\n",
    "print(\"  • Alpha (step size): 2/255 ≈ 0.008\")\n",
    "print(\"  • Num steps: 7 iterations\")\n",
    "print(\"  • Expected: Stronger robustness than FGSM\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867681fb",
   "metadata": {},
   "source": [
    "## CELL 5: Create DataLoaders & Multi-Stage Fine-Tuning (RESUME-SAFE)\n",
    "\n",
    "Train ViT + CBAM with PGD adversarial examples.  \n",
    "**CRITICAL**: Checks if adversarial checkpoint already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae599de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADVERSARIAL FINE-TUNING WITH CBAM\n",
      "================================================================================\n",
      "No existing adversarial checkpoint found.\n",
      "Starting multi-stage fine-tuning with PGD adversarial training...\n",
      "This will take approximately 2 hours.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CREATING DATALOADERS\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Train loader: 1659 batches\n",
      "✓ Val loader: 356 batches\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING CONFIGURATION\n",
      "--------------------------------------------------------------------------------\n",
      "Epochs: 10\n",
      "Learning Rate: 1e-05\n",
      "Batch Size: 32\n",
      "Optimizer: AdamW\n",
      "Scheduler: CosineAnnealing\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STAGE 1: FREEZE BACKBONE, TRAIN CBAM + CLASSIFIER (Epochs 1-3)\n",
      "--------------------------------------------------------------------------------\n",
      "Trainable parameters: 0.1M\n",
      "\n",
      "Epoch 1/3 (Stage 1)\n",
      "------------------------------------------------------------\n",
      "  Batch 200: Loss=1.2687, CleanAcc=100.00%, AdvAcc=15.62%\n",
      "  Batch 400: Loss=1.3931, CleanAcc=100.00%, AdvAcc=3.12%\n",
      "  Batch 600: Loss=1.4020, CleanAcc=100.00%, AdvAcc=3.12%\n",
      "  Batch 800: Loss=1.2307, CleanAcc=100.00%, AdvAcc=15.62%\n",
      "  Batch 1000: Loss=1.2142, CleanAcc=100.00%, AdvAcc=6.25%\n",
      "  Batch 1200: Loss=1.2775, CleanAcc=100.00%, AdvAcc=12.50%\n",
      "  Batch 1400: Loss=1.2716, CleanAcc=100.00%, AdvAcc=15.62%\n",
      "  Batch 1600: Loss=1.2172, CleanAcc=100.00%, AdvAcc=18.75%\n",
      "Epoch 1: Train Loss=1.2334, Val Acc=99.65%\n",
      "\n",
      "Epoch 2/3 (Stage 1)\n",
      "------------------------------------------------------------\n",
      "  Batch 200: Loss=1.0888, CleanAcc=100.00%, AdvAcc=21.88%\n",
      "  Batch 400: Loss=1.3698, CleanAcc=100.00%, AdvAcc=12.50%\n",
      "  Batch 600: Loss=1.1764, CleanAcc=100.00%, AdvAcc=15.62%\n",
      "  Batch 800: Loss=1.2171, CleanAcc=96.88%, AdvAcc=21.88%\n",
      "  Batch 1000: Loss=1.1481, CleanAcc=96.88%, AdvAcc=12.50%\n",
      "  Batch 1200: Loss=1.2065, CleanAcc=100.00%, AdvAcc=12.50%\n",
      "  Batch 1400: Loss=0.9520, CleanAcc=100.00%, AdvAcc=43.75%\n",
      "  Batch 1600: Loss=1.1080, CleanAcc=96.88%, AdvAcc=12.50%\n",
      "Epoch 2: Train Loss=1.1456, Val Acc=99.69%\n",
      "\n",
      "Epoch 3/3 (Stage 1)\n",
      "------------------------------------------------------------\n",
      "  Batch 200: Loss=1.0718, CleanAcc=100.00%, AdvAcc=18.75%\n",
      "  Batch 400: Loss=1.0428, CleanAcc=100.00%, AdvAcc=25.00%\n",
      "  Batch 600: Loss=1.0739, CleanAcc=100.00%, AdvAcc=21.88%\n",
      "  Batch 800: Loss=1.1255, CleanAcc=96.88%, AdvAcc=25.00%\n",
      "  Batch 1000: Loss=1.0807, CleanAcc=100.00%, AdvAcc=21.88%\n",
      "  Batch 1200: Loss=1.0131, CleanAcc=100.00%, AdvAcc=21.88%\n",
      "  Batch 1400: Loss=1.1713, CleanAcc=100.00%, AdvAcc=25.00%\n",
      "  Batch 1600: Loss=1.1888, CleanAcc=100.00%, AdvAcc=21.88%\n",
      "Epoch 3: Train Loss=1.0944, Val Acc=99.75%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STAGE 2: UNFREEZE LAST 3 LAYERS (Epochs 4-10)\n",
      "--------------------------------------------------------------------------------\n",
      "Trainable parameters: 21.3M\n",
      "\n",
      "Epoch 4/10 (Stage 2)\n",
      "------------------------------------------------------------\n",
      "  Batch 200: Loss=0.9286, CleanAcc=100.00%, AdvAcc=34.38%\n",
      "  Batch 400: Loss=0.9908, CleanAcc=100.00%, AdvAcc=28.12%\n",
      "  Batch 600: Loss=0.9629, CleanAcc=100.00%, AdvAcc=37.50%\n",
      "  Batch 800: Loss=0.8748, CleanAcc=100.00%, AdvAcc=31.25%\n",
      "  Batch 1000: Loss=0.8900, CleanAcc=96.88%, AdvAcc=28.12%\n",
      "  Batch 1200: Loss=0.7266, CleanAcc=100.00%, AdvAcc=53.12%\n",
      "  Batch 1400: Loss=0.8351, CleanAcc=100.00%, AdvAcc=40.62%\n",
      "  Batch 1600: Loss=0.7094, CleanAcc=100.00%, AdvAcc=50.00%\n",
      "Epoch 4: Train Loss=0.8886, Val Acc=99.78%, LR=5.00e-06\n",
      "  ✓ Best model saved (val_acc=99.78%)\n",
      "\n",
      "Epoch 5/10 (Stage 2)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 226\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    224\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 226\u001b[0m     loss_clean, loss_adv, loss_total, acc_clean, acc_adv \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_with_pgd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_total\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[4], line 102\u001b[0m, in \u001b[0;36mtrain_step_with_pgd\u001b[0;34m(model, images, labels, optimizer, eps, alpha, num_steps)\u001b[0m\n\u001b[1;32m     99\u001b[0m loss_clean \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(outputs_clean, labels)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Generate adversarial examples\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m adv_images \u001b[38;5;241m=\u001b[39m \u001b[43mpgd_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Forward pass on adversarial images\u001b[39;00m\n\u001b[1;32m    105\u001b[0m outputs_adv \u001b[38;5;241m=\u001b[39m model(adv_images)\n",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m, in \u001b[0;36mpgd_attack\u001b[0;34m(model, images, labels, eps, alpha, num_steps)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(outputs, labels)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Get gradient sign and take step in that direction\u001b[39;00m\n\u001b[1;32m     60\u001b[0m grad \u001b[38;5;241m=\u001b[39m adv_images\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/miniconda3/envs/major project/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/major project/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/major project/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: ADVERSARIAL FINE-TUNING (RESUME-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "Multi-stage fine-tuning strategy:\n",
    "1. Stage 1 (Epochs 1-3): Freeze ViT backbone, train CBAM + classifier only\n",
    "2. Stage 2 (Epochs 4-10): Unfreeze last 3 transformer layers, full fine-tuning\n",
    "\n",
    "Uses PGD adversarial training for robustness.\n",
    "\n",
    "RESUME-SAFE: Checks if adversarial checkpoint exists before training.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVERSARIAL FINE-TUNING WITH CBAM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# CHECK IF ALREADY TRAINED (RESUME-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "adv_checkpoint_path = base_dir / 'checkpoints' / 'adversarial_finetuned.pt'\n",
    "\n",
    "if adv_checkpoint_path.exists():\n",
    "    print(\"✓ Found existing adversarial checkpoint, loading...\")\n",
    "    checkpoint = torch.load(adv_checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    val_acc = checkpoint['val_accuracy']\n",
    "    print(f\"✓ Loaded adversarial model (val_acc={val_acc:.2f}%)\")\n",
    "    print(\"Skipping training (resume-safe)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No existing adversarial checkpoint found.\")\n",
    "    print(\"Starting multi-stage fine-tuning with PGD adversarial training...\")\n",
    "    print(\"This will take approximately 2 hours.\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # CREATE DATALOADERS\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CREATING DATALOADERS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Reload dataset samples\n",
    "    class HMDB51FightDataset(Dataset):\n",
    "        def __init__(self, root_dir, split, class_to_idx):\n",
    "            self.root_dir = Path(root_dir)\n",
    "            self.samples = []\n",
    "            split_dir = self.root_dir / split\n",
    "            for class_name, class_idx in class_to_idx.items():\n",
    "                class_dir = split_dir / class_name\n",
    "                if class_dir.exists():\n",
    "                    for img_path in class_dir.glob('*.jpg'):\n",
    "                        self.samples.append({\n",
    "                            'path': str(img_path),\n",
    "                            'label': class_idx,\n",
    "                            'class_name': class_name\n",
    "                        })\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "    \n",
    "    train_dataset_loader = HMDB51FightDataset(CONFIG['dataset_path'], 'train', class_to_idx)\n",
    "    test_dataset_loader = HMDB51FightDataset(CONFIG['dataset_path'], 'test', class_to_idx)\n",
    "    all_samples = train_dataset_loader.samples + test_dataset_loader.samples\n",
    "    \n",
    "    class HMDB51Dataset(Dataset):\n",
    "        def __init__(self, samples, indices, transform=None):\n",
    "            self.samples = [samples[i] for i in indices]\n",
    "            self.transform = transform\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "        def __getitem__(self, idx):\n",
    "            sample = self.samples[idx]\n",
    "            img = Image.open(sample['path']).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img, sample['label']\n",
    "    \n",
    "    # Transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = HMDB51Dataset(all_samples, train_indices, transform=train_transform)\n",
    "    val_dataset = HMDB51Dataset(all_samples, val_indices, transform=val_transform)\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                           num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"✓ Train loader: {len(train_loader)} batches\")\n",
    "    print(f\"✓ Val loader: {len(val_loader)} batches\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # TRAINING CONFIGURATION\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING CONFIGURATION\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-5  # Lower LR for fine-tuning\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Learning Rate: {learning_rate}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"Optimizer: AdamW\")\n",
    "    print(f\"Scheduler: CosineAnnealing\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # MULTI-STAGE FINE-TUNING\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STAGE 1: FREEZE BACKBONE, TRAIN CBAM + CLASSIFIER (Epochs 1-3)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Freeze ViT backbone\n",
    "    for param in model.vit.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # CBAM and classifier are trainable\n",
    "    for param in model.cbam.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable/1e6:.1f}M\")\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(3):  # Stage 1: 3 epochs\n",
    "        print(f\"\\nEpoch {epoch+1}/3 (Stage 1)\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Train with PGD adversarial examples\n",
    "            loss_clean, loss_adv, loss_total, acc_clean, acc_adv = train_step_with_pgd(\n",
    "                model, images, labels, optimizer, eps=8/255, alpha=2/255, num_steps=7\n",
    "            )\n",
    "            \n",
    "            epoch_loss += loss_total\n",
    "            \n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}: Loss={loss_total:.4f}, CleanAcc={acc_clean:.2%}, AdvAcc={acc_adv:.2%}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    # ============================================================\n",
    "    # STAGE 2: UNFREEZE LAST LAYERS\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STAGE 2: UNFREEZE LAST 3 LAYERS (Epochs 4-10)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Unfreeze last 3 transformer blocks\n",
    "    for block in model.vit.blocks[-3:]:\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable/1e6:.1f}M\")\n",
    "    \n",
    "    # Create new optimizer for stage 2\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate/2, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=7)\n",
    "    \n",
    "    for epoch in range(3, num_epochs):  # Stage 2: epochs 4-10\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} (Stage 2)\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss_clean, loss_adv, loss_total, acc_clean, acc_adv = train_step_with_pgd(\n",
    "                model, images, labels, optimizer, eps=8/255, alpha=2/255, num_steps=7\n",
    "            )\n",
    "            \n",
    "            epoch_loss += loss_total\n",
    "            \n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                print(f\"  Batch {batch_idx+1}: Loss={loss_total:.4f}, CleanAcc={acc_clean:.2%}, AdvAcc={acc_adv:.2%}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler.step\n",
    "        ()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Val Acc={val_acc:.2f}%, LR={scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_acc,\n",
    "                'history': history,\n",
    "            }, adv_checkpoint_path)\n",
    "            print(f\"  ✓ Best model saved (val_acc={val_acc:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ADVERSARIAL FINE-TUNING COMPLETE\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea900ee",
   "metadata": {},
   "source": [
    "## CELL 6: Notebook 6 Summary & Next Steps\n",
    "\n",
    "Summary of adversarial fine-tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9145e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NOTEBOOK 6: ADVERSARIAL FINE-TUNING - COMPLETION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Adversarial Fine-Tuning Complete:\n",
      "  • Final validation accuracy: 99.78%\n",
      "  • Checkpoint: novelty_files/checkpoints/adversarial_finetuned.pt\n",
      "\n",
      "✓ Techniques Applied:\n",
      "  • CBAM Attention (Channel + Spatial)\n",
      "  • PGD Adversarial Training (7 steps)\n",
      "  • Multi-stage fine-tuning\n",
      "  • Gradient clipping & regularization\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ACCURACY PROGRESSION\n",
      "--------------------------------------------------------------------------------\n",
      "Baseline (VGG-16):              71.0%\n",
      "ViT-Base (Notebook 2):          ~80%\n",
      "DDP + NSL (Notebook 4):         ~99.8%\n",
      "Adversarial + CBAM (Notebook 6): 99.78%\n",
      "\n",
      "✓ Target (87-90%): ACHIEVED\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "NEXT STEPS\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Notebook 6 COMPLETE: Adversarial Fine-Tuning\n",
      "\n",
      "Ready to proceed to:\n",
      "  → Notebook 7: Ablation Studies\n",
      "     • Test 15+ configuration combinations\n",
      "     • Measure contribution of each component\n",
      "     • Generate comparison table\n",
      "     • Runtime: ~24-36 hours\n",
      "\n",
      "================================================================================\n",
      "NOTEBOOK 6: ✓ SUCCESSFULLY COMPLETED\n",
      "================================================================================\n",
      "\n",
      "✓ Completion status saved to: novelty_files/logs/notebook_06_completion.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: NOTEBOOK 6 COMPLETION SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTEBOOK 6: ADVERSARIAL FINE-TUNING - COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verify checkpoint exists\n",
    "if adv_checkpoint_path.exists():\n",
    "    checkpoint = torch.load(adv_checkpoint_path, map_location='cpu')\n",
    "    final_val_acc = checkpoint['val_accuracy']\n",
    "    \n",
    "    print(\"\\n✓ Adversarial Fine-Tuning Complete:\")\n",
    "    print(f\"  • Final validation accuracy: {final_val_acc:.2f}%\")\n",
    "    print(f\"  • Checkpoint: {adv_checkpoint_path}\")\n",
    "    \n",
    "    print(\"\\n✓ Techniques Applied:\")\n",
    "    print(\"  • CBAM Attention (Channel + Spatial)\")\n",
    "    print(\"  • PGD Adversarial Training (7 steps)\")\n",
    "    print(\"  • Multi-stage fine-tuning\")\n",
    "    print(\"  • Gradient clipping & regularization\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"ACCURACY PROGRESSION\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"Baseline (VGG-16):              71.0%\")\n",
    "    print(\"ViT-Base (Notebook 2):          ~80%\")\n",
    "    print(\"DDP + NSL (Notebook 4):         ~99.8%\")\n",
    "    print(f\"Adversarial + CBAM (Notebook 6): {final_val_acc:.2f}%\")\n",
    "    \n",
    "    target_met = final_val_acc >= 87.0\n",
    "    print(f\"\\n{'✓' if target_met else '✗'} Target (87-90%): {'ACHIEVED' if target_met else 'NOT MET'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"NEXT STEPS\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"✓ Notebook 6 COMPLETE: Adversarial Fine-Tuning\")\n",
    "    print(\"\\nReady to proceed to:\")\n",
    "    print(\"  → Notebook 7: Ablation Studies\")\n",
    "    print(\"     • Test 15+ configuration combinations\")\n",
    "    print(\"     • Measure contribution of each component\")\n",
    "    print(\"     • Generate comparison table\")\n",
    "    print(\"     • Runtime: ~24-36 hours\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NOTEBOOK 6: ✓ SUCCESSFULLY COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save completion status\n",
    "    completion_status = {\n",
    "        'notebook': 'Notebook 6: Adversarial Fine-Tuning',\n",
    "        'completed': True,\n",
    "        'timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'val_accuracy': float(final_val_acc),\n",
    "        'target_met': bool(target_met),\n",
    "        'techniques': [\n",
    "            'CBAM Attention',\n",
    "            'PGD Adversarial Training',\n",
    "            'Multi-stage Fine-tuning'\n",
    "        ],\n",
    "        'checkpoint': str(adv_checkpoint_path)\n",
    "    }\n",
    "    \n",
    "    completion_path = base_dir / 'logs' / 'notebook_06_completion.json'\n",
    "    with open(completion_path, 'w') as f:\n",
    "        json.dump(completion_status, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Completion status saved to: {completion_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ Adversarial checkpoint not found. Please run the training cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79fb1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "major project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
